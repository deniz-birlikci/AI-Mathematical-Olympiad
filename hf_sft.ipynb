{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config = {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "In this section, we are defining an abstract base class named 'Model'. This class serves as a template for creating various model instances that can be easily interchanged for benchmarking purposes. By adhering to the interface specified by this abstract class, different models can be integrated into our workflow with minimal changes to the surrounding code. This design allows for a consistent way to load, prompt, perform inference, and retrieve results across different models, facilitating a more streamlined comparison of their performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from collections import Counter\n",
    "\n",
    "class Model(ABC):\n",
    "    def __init__(self, self_consistency_reps=10):\n",
    "        self.self_consistency_reps = self_consistency_reps\n",
    "        \n",
    "        self.model = None\n",
    "        self.device = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def format_prompt(self, question):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def inference(self, prompt):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def retrieve_result(self, response):\n",
    "        pass\n",
    "    \n",
    "    # TODO: Implement code execution loop somewhere\n",
    "    def predict(self, question):\n",
    "        prompt = self.format_prompt(question)\n",
    "        output = self.inference(prompt)\n",
    "        result = self.retrieve_result(output)\n",
    "        return result\n",
    "    \n",
    "    # TODO: Make this function more efficient by running the SC as a batch instead\n",
    "    # TODO: Add another implementation here when we are running the code blocks from the model and evaluating both the response given by the model and the response given by the code block\n",
    "    def predict_self_consistent(self, question):\n",
    "        answers = []\n",
    "        for _ in range(self.self_consistency_reps):\n",
    "            answers.append(self.predict(question))\n",
    "        most_common_answer = Counter(answers).most_common(1)[0][0]\n",
    "        return most_common_answer\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixtral(Model):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface with the competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test(test_csv_path):\n",
    "    answers = None\n",
    "\n",
    "    pass\n",
    "    # TODO: return answers as a DF\n",
    "    # TODO: save answers as a submission CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
