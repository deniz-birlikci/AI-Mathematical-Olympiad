{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaQu8zS7ceGq",
        "outputId": "a01480af-20ba-420f-c7c1-0c9e3c89c2e1"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXiSt3BVimVk"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "93i2kuT_cpdJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from datasets import load_dataset\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re, sys, subprocess, gc\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5xHheeaioS-"
      },
      "source": [
        "### Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C37K6PxudeHd"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/docs/peft/main/en/developer_guides/quantization\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "b504083dc64644c98b52f29eb5e5bb3e",
            "4ba8743d740c4656ac9e841e8f2db15b",
            "9ad7c3eda9c74acf8e90daaba990c43f",
            "3ab6195eab284a8bac0cbd2fd5b55017",
            "babffd6d16a3473693b6095afc33b73d",
            "9c71f7dd2cf44eee924b26bc48e97b7c",
            "6643557d2f6a4a629863d38b3c79a91f",
            "77c6492bbae843e5a5f44b484a65bd39",
            "33b35d9d7abf4902a295fc7190e0c651",
            "09542cd524504225a0b71aa333dc1bd2",
            "69cb0fd7074a4d00abc89bb9a09b753e"
          ]
        },
        "id": "kxyfHIlAeDYc",
        "outputId": "1197f197-6fd2-446e-ddd3-24e6b625fd27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mps\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "827da67f8f7a427fa8b1afa1cfc6d8a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load in the model\n",
        "model_id = \"nvidia/OpenMath-Mistral-7B-v0.1-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"cuda\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, \n",
        "                                                device_map=\"cuda:0\", \n",
        "                                                quantization_config=quantization_config, \n",
        "                                                pad_token_id=tokenizer.eos_token_id)\n",
        "elif torch.backends.mps.is_available():\n",
        "    print(\"mps\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                                 device_map=\"mps\",\n",
        "                                                 pad_token_id=tokenizer.eos_token_id)\n",
        "else:\n",
        "    print(\"cpu\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                                 pad_token_id=tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "print(tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer, torch_dtype='auto', device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VDzPW8OOohxX"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_eos_token = True\n",
        "tokenizer.add_bos_token, tokenizer.add_eos_token\n",
        "tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cBGfR01sfPEg"
      },
      "outputs": [],
      "source": [
        "#model = prepare_model_for_kbit_training(quantized_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ck5TVeDFgCs0"
      },
      "outputs": [],
      "source": [
        "# lora_config = LoraConfig(\n",
        "#     r=16,\n",
        "#     lora_alpha=8,\n",
        "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "#     lora_dropout=0.05,\n",
        "#     bias=\"none\",\n",
        "#     task_type=\"CAUSAL_LM\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xMKbbkBSgMVl"
      },
      "outputs": [],
      "source": [
        "# model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exUrf6bHiqQu"
      },
      "source": [
        "### Datahandling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "kqaw1BjOirte"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import random\n",
        "from format_data import get_data_SFTTrainer\n",
        "from rag import get_RAG_context\n",
        "# from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "gPwHW5NniuNu"
      },
      "outputs": [],
      "source": [
        "train_dataset = get_data_SFTTrainer('data/math/merged_math_problems_train_clean.json')\n",
        "random.shuffle(train_dataset)\n",
        "\n",
        "split = 0.8\n",
        "train_dataset = train_dataset[:int(split*len(train_dataset))]\n",
        "val_dataset = train_dataset[int(split*len(train_dataset)):]\n",
        "test_dataset = get_data_SFTTrainer('data/aime_clean.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_5zx28umslj",
        "outputId": "6ccd2506-86d8-42d4-af50-3a5a4d656952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'prompt': 'Five men and nine women stand equally spaced around a circle in random order. The probability that every man stands diametrically opposite a woman is <math> LaTex frac{m}{n},</math> where <math>m</math> and <math>n</math> are relatively prime positive integers. Find <math>m+n.</math>', 'completion': '\\nFor simplicity purposes, we consider two arrangements different even if they only differ by rotations or reflections. In this way, there are <math>14!</math> arrangements without restrictions.\\n\\nFirst, there are <math> LaTex binom{7}{5}</math> ways to choose the man-woman diameters. Then, there are <math>10 LaTex cdot8 LaTex cdot6 LaTex cdot4 LaTex cdot2</math> ways to place the five men each in a man-woman diameter. Finally, there are <math>9!</math> ways to place the nine women without restrictions.\\n\\nTogether, the requested probability is <cmath> LaTex frac{ LaTex tbinom{7}{5} LaTex cdot(10 LaTex cdot8 LaTex cdot6 LaTex cdot4 LaTex cdot2) LaTex cdot9!}{14!} =  LaTex frac{21 LaTex cdot(10 LaTex cdot8 LaTex cdot6 LaTex cdot4 LaTex cdot2)}{14 LaTex cdot13 LaTex cdot12 LaTex cdot11 LaTex cdot10} =  LaTex frac{48}{143},</cmath> from which the answer is <math>48+143 =  LaTex boxed{191}.</math>\\n\\n~MRENTHUSIASM\\n'}\n"
          ]
        }
      ],
      "source": [
        "print(test_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "vUEsIkTwlMC6"
      },
      "outputs": [],
      "source": [
        "#TODO: Additional prompt engineering to clean up just appending RAG context to the start\n",
        "text_format = \"\"\"### Question: {problem_statement}\"\"\"\n",
        "\n",
        "# RAG support\n",
        "prompts = [example[\"prompt\"] for example in train_dataset] \n",
        "vectorizer = TfidfVectorizer()                       # Initialize TF-IDF vectorizer\n",
        "tfidf_matrix = vectorizer.fit_transform(prompts)     # Fit vectorizer on prompts\n",
        "RAG_params = {\n",
        "    \"train_data\": train_dataset,\n",
        "    \"vectorizer\": vectorizer,\n",
        "    \"tfidf_matrix\": tfidf_matrix,\n",
        "}\n",
        "TOP_K = 1\n",
        "\n",
        "def formatting_prompts_func(dataset):\n",
        "    output_texts = []\n",
        "    for question_dict in tqdm(dataset):\n",
        "        problem = question_dict[\"prompt\"]\n",
        "        text = text_format.format(problem_statement=problem)\n",
        "        # get RAG-supported context\n",
        "        context = get_RAG_context(question_dict, top_k=TOP_K, RAG_params = RAG_params)\n",
        "        output_texts.append(context+text)\n",
        "    return output_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "3FALzpw3nfi-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 975/975 [00:01<00:00, 638.97it/s]\n"
          ]
        }
      ],
      "source": [
        "rag_formatted_test_dataset = formatting_prompts_func(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['### prompt: The probability that Kim has a math test today is $ LaTex frac{4}{7}$. What is the probability that Kim does not have a math test today? Express your answer as a common fraction. [completion]: The probability that Kim does not have a math test is equal to one minus the probability she does have a math test. So, the probability of not having a math test is $1 -  LaTex frac{4}{7} =  LaTex boxed{ LaTex frac{3}{7}}$.### Question: Five men and nine women stand equally spaced around a circle in random order. The probability that every man stands diametrically opposite a woman is <math> LaTex frac{m}{n},</math> where <math>m</math> and <math>n</math> are relatively prime positive integers. Find <math>m+n.</math> Please give your answer as an integer between 0 and 999.',\n",
              " \"### prompt: In how many ways can I arrange 3 different math books and 5 different history books on my bookshelf, if I require there to be a math book on both ends? [completion]: Let's deal with the restriction first.\\n\\nThe restriction is that we have to place a math book on both ends.  We have 3 choices for the math book to place on the left end, and then 2 choices for the math book to place on the right end.\\n\\nThen we simply need to arrange the other 6 books in the middle.  This is a basic permutation problem, so there are $6!$ ways to arrange the 6 remaining books.\\n\\nSo there are a total of $3  LaTex times 2  LaTex times 6! =  LaTex boxed{4, LaTex !320}$ ways to arrange the books on the bookshelf.### Question: Positive real numbers <math>b  LaTex not= 1</math> and <math>n</math> satisfy the equations <cmath> LaTex sqrt{ LaTex log_b n} =  LaTex log_b  LaTex sqrt{n}  LaTex qquad  LaTex text{and}  LaTex qquad b  LaTex cdot  LaTex log_b n =  LaTex log_b (bn).</cmath> The value of <math>n</math> is <math> LaTex frac{j}{k},</math> where <math>j</math> and <math>k</math> are relatively prime positive integers. Find <math>j+k.</math> Please give your answer as an integer between 0 and 999.\"]"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_formatted_test_dataset[0:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Answer Extraction Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "def naive_parse(answer):\n",
        "    out = []\n",
        "    start = False\n",
        "    end = False\n",
        "    for l in reversed(list(answer)):\n",
        "        if l in '0123456789' and not end:\n",
        "            start = True\n",
        "            out.append(l)\n",
        "        else:\n",
        "            if start:\n",
        "                end = True\n",
        "        \n",
        "    out = reversed(out)\n",
        "    return ''.join(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_output(output): \n",
        "    result = output\n",
        "    try:\n",
        "        code = output.split('```')[1][7:]\n",
        "        with open('code.py', 'w') as fout:\n",
        "            fout.write(code)\n",
        "        batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n",
        "        try:\n",
        "            shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n",
        "            code_output = round(float(eval(shell_output))) % 1000\n",
        "        except:\n",
        "            code_output = -1\n",
        "    except Exception as e:\n",
        "        code_output = -1\n",
        "    try:\n",
        "        result_output = re.findall(r'\\\\boxed\\{(.*)\\}', result)\n",
        "        if not len(result_output):\n",
        "            result_output = naive_parse(result)\n",
        "        else:\n",
        "            result_output = result_output[-1]\n",
        "        if not len(result_output):\n",
        "            result_output = -1\n",
        "        else:\n",
        "            result_output = round(float(eval(result_output))) % 1000\n",
        "    except Exception as e:\n",
        "        result_output = -1\n",
        "    return result_output, code_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "  1%|          | 1/100 [01:04<1:46:32, 64.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "  2%|▏         | 2/100 [01:57<1:34:37, 57.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "  3%|▎         | 3/100 [02:09<59:07, 36.58s/it]  Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "  4%|▍         | 4/100 [02:19<41:49, 26.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "  5%|▌         | 5/100 [02:29<32:07, 20.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "  6%|▌         | 6/100 [02:39<26:23, 16.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "tool_instruction = \" The answer should be given as a non-negative modulo 1000.\"\n",
        "tool_instruction += '\\nPlease integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}.'\n",
        "n_repetitions = 5\n",
        "total_results = []\n",
        "total_answers = []\n",
        "\n",
        "for i in tqdm(range(len(rag_formatted_test_dataset[:100]))):\n",
        "    problem = rag_formatted_test_dataset[i]\n",
        "    messages = [{\"role\": \"user\", \"content\": problem + tool_instruction}]\n",
        "    query_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    results = []\n",
        "    answers = []\n",
        "    for _ in range(n_repetitions):\n",
        "        try:\n",
        "            raw_output = pipeline(query_prompt, max_new_tokens=2048, do_sample=True, temperature=0.8911, return_full_text=False)\n",
        "            raw_output = raw_output[0]['generated_text']\n",
        "            #print(raw_output)\n",
        "            result_output, code_output = process_output(raw_output)\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(\"caught error:\", e)\n",
        "            result_output, code_output = -1, -1\n",
        "        results.append(result_output)\n",
        "        answers.append(code_output)\n",
        "    total_results.append(results)\n",
        "    total_answers.append(answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]]\n",
            "[[0], [-1], [-1], [2], [-1], [-1], [-1], [-1], [56], [-1]]\n"
          ]
        }
      ],
      "source": [
        "print(total_answers)\n",
        "print(total_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### prompt: We roll a fair 6-sided die 5 times.  What is the probability that we get an odd number in exactly 4 of the 5 rolls? [completion]: The chances of getting an odd or even number are equal, so there are $2^5=32$ equally likely outcomes.  If we want to get exactly 4 of 5 the rolls to be odd, the probability is $ LaTex dfrac{ LaTex binom{5}{4}}{2^5}= LaTex boxed{ LaTex dfrac{5}{32}}.$### Question: We roll a fair 6-sided die 5 times.  What is the probability that we get a 6 in at most 2 of the rolls?\n",
            "[{'generated_text': '\\nWe can count the number of outcomes by the casework argument. There are 3 cases to consider: no 6s, exactly 1 6, and exactly 2 6s.\\n<llm-code>\\nfrom sympy import binomial\\n\\nn = 5\\np = 6\\n\\n# no 6s\\nnum_no_6s = binomial(n, 0)\\n\\n# exactly 1 6\\nnum_1_6s = binomial(n, 1)\\n\\n# exactly 2 6s\\nnum_2_6s = binomial(n, 2)\\n\\n# probability\\nprob = (num_no_6s + num_1_6s + num_2_6s) / 2**n\\nprint(prob)\\n</llm-code>\\n<llm-code-output>\\n11/32\\n</llm-code-output>\\nThe probability of getting a 6 in at most 2 of the rolls is $ LaTex boxed{ LaTex dfrac{11}{32}}.$'}]\n"
          ]
        }
      ],
      "source": [
        "problem = rag_formatted_test_dataset[0]\n",
        "# problem = test_dataset[0]['prompt']\n",
        "print(problem)\n",
        "\n",
        "# token_input = tokenizer.encode(query_prompt, return_tensors='pt')\n",
        "# output = model.generate(token_input, max_length=5000, num_return_sequences=1, temperature=0.7)\n",
        "# generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "# print(token_input[-1])\n",
        "# print(generated_text)\n",
        "\n",
        "print(pipeline(problem, max_new_tokens=2048, do_sample=True, temperature=0.8911, return_full_text=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0] [0]\n",
            "[-1] [-1]\n",
            "[-1] [-1]\n",
            "[2] [2]\n",
            "[-1] [-1]\n",
            "[-1] [-1]\n",
            "[-1] [-1]\n",
            "[-1] [-1]\n",
            "[56] [56]\n",
            "[-1] [-1]\n"
          ]
        }
      ],
      "source": [
        "final_answers = []\n",
        "for a, b in zip(total_answers, total_results):\n",
        "    a = np.array(a)\n",
        "    b = np.array(b)\n",
        "    a[a < 0] = b[a < 0]\n",
        "    pred = Counter(a.tolist()).most_common(2)\n",
        "    #REMOVE -1 Answers?\n",
        "    try:\n",
        "        ans = pred[0][0] if not pred[0][0] < 0 else pred[1][0]\n",
        "    except:\n",
        "        ans = pred[0][0]\n",
        "    while ans > 999: ans = ans % 1000\n",
        "    final_answers.append(ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_ground_truth(output): \n",
        "    result = output\n",
        "    try:\n",
        "        result_output = re.findall(r'\\\\boxed\\{(.*)\\}', result)\n",
        "        if not len(result_output):\n",
        "            result_output = naive_parse(result)\n",
        "        else:\n",
        "            result_output = result_output[-1]\n",
        "        if not len(result_output):\n",
        "            result_output = -1\n",
        "        else:\n",
        "            result_output = round(float(eval(result_output))) % 1000\n",
        "    except Exception as e:\n",
        "        result_output = -1\n",
        "    return result_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 31068.92it/s]\n"
          ]
        }
      ],
      "source": [
        "parsed_ground_truth = []\n",
        "\n",
        "for i in tqdm(range(len(final_answers))):\n",
        "    parsed_ground_truth.append(process_ground_truth(test_dataset[i][\"completion\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "[648, 149, 5, 48, 72, 6, 10, 23, 560, 7]\n",
            "[648, -1, -1, 2, -1, -1, -1, -1, 56, -1]\n",
            "Accuracy: 0.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "correct = np.sum(np.array(final_answers) == np.array(parsed_ground_truth))\n",
        "\n",
        "print(\"Accuracy:\",correct/len(final_answers))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09542cd524504225a0b71aa333dc1bd2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17fea4b951df466cbc2b7f54df3cb040": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c0df00f46194588af2102ee467b385a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e03166105404f998995705f1ae74e73",
            "placeholder": "​",
            "style": "IPY_MODEL_30830853581d4c1b906d514e245f2fcf",
            "value": "Map: 100%"
          }
        },
        "1f2b6554b4214c8b93af02421d4e79ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c0df00f46194588af2102ee467b385a",
              "IPY_MODEL_f9357ab73be9400dadade883bd6d3ee0",
              "IPY_MODEL_b7c654f1b3344c18a4ce3e8bb93d7fea"
            ],
            "layout": "IPY_MODEL_17fea4b951df466cbc2b7f54df3cb040"
          }
        },
        "30830853581d4c1b906d514e245f2fcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33b35d9d7abf4902a295fc7190e0c651": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ab6195eab284a8bac0cbd2fd5b55017": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09542cd524504225a0b71aa333dc1bd2",
            "placeholder": "​",
            "style": "IPY_MODEL_69cb0fd7074a4d00abc89bb9a09b753e",
            "value": " 2/2 [00:10&lt;00:00,  4.77s/it]"
          }
        },
        "4ac3575b38e44432ace925cf9443dada": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ba8743d740c4656ac9e841e8f2db15b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c71f7dd2cf44eee924b26bc48e97b7c",
            "placeholder": "​",
            "style": "IPY_MODEL_6643557d2f6a4a629863d38b3c79a91f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6643557d2f6a4a629863d38b3c79a91f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69cb0fd7074a4d00abc89bb9a09b753e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "706f6af69f114e31ac5d4469e39e90a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77c6492bbae843e5a5f44b484a65bd39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ad7c3eda9c74acf8e90daaba990c43f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77c6492bbae843e5a5f44b484a65bd39",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33b35d9d7abf4902a295fc7190e0c651",
            "value": 2
          }
        },
        "9c71f7dd2cf44eee924b26bc48e97b7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e03166105404f998995705f1ae74e73": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaffe8a85ad543a18df1b7c17edee79a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b504083dc64644c98b52f29eb5e5bb3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ba8743d740c4656ac9e841e8f2db15b",
              "IPY_MODEL_9ad7c3eda9c74acf8e90daaba990c43f",
              "IPY_MODEL_3ab6195eab284a8bac0cbd2fd5b55017"
            ],
            "layout": "IPY_MODEL_babffd6d16a3473693b6095afc33b73d"
          }
        },
        "b7c654f1b3344c18a4ce3e8bb93d7fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_706f6af69f114e31ac5d4469e39e90a7",
            "placeholder": "​",
            "style": "IPY_MODEL_4ac3575b38e44432ace925cf9443dada",
            "value": " 7500/7500 [00:01&lt;00:00, 4862.01 examples/s]"
          }
        },
        "babffd6d16a3473693b6095afc33b73d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9d6e0a60a324dab8c10fb09560911f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9357ab73be9400dadade883bd6d3ee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9d6e0a60a324dab8c10fb09560911f5",
            "max": 7500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aaffe8a85ad543a18df1b7c17edee79a",
            "value": 7500
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
